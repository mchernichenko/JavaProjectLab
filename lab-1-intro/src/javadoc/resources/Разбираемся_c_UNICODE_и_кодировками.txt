Кодовые точки Unicode и русские символы в исходных кодах и программах Java.
Здесь толковая статья: http://strongexperts.narod.ru/ru/articles/archive/java2/2006/nov2006-001/nov2006-001.htm

Про русские буквы в Java: http://www.getinfo.ru/article296.html
					      http://www.javaportal.ru/java/articles/UnicodeJDK1_6.html
						  http://study-and-dev.com/blog/java_ru_1/
=========================================================================
Unicode - это не кодировка, а стандарт кодирования символов, обеспечивающий цифровое представление символов всех письменностей мира и специальных символов.

Каждому символу (который вообще может быть, хоть китайский иероглиф) в Unicode присвоено волшебное число (цифровое представление) - КОДОВАЯ ТОЧКА
Т.е. он лишь определяет связь между символом и некоторым абстрактным числом, которое в компьютере может представляться множеством разных способов, зависит от кодировки 
Иными словами, формат, согласно с которым эти числа будут превращаться в байты, определяются Юникод-кодировками (например, UTF-8 или UTF-16).

Коды символов ASCII и коды в Unicode совпадают (первые 128 символов, от U+0000 до U+007F, 7*16^1+15*16^0=127), далее, например, русская "А" в Unicode 0x0410 (или 1040), а в Cp866 - имеет код 128, а в Win1251 - имеет код 192


Все пространство кодовых точек это диапазон чисел: 0x000000-0x10FFFF (за минусом 2048 зарезервированных значений, итого 1.114.111-2048 = 1.112.064 значения доступно для кодирования unicod`ом)
Почему именно так, см. кодировку UTF-16. Именно для совместимости с UTF-16 ограничились лишь 1 112 064, хотя формы записи UTF-8 и UTF-32 позволяют кодировать до 231 (2 147 483 648) кодовых позиций
Но и 1 112 064 сочли докуя и решили ограничиться.
 
Диапазон 0x000000-0x10FFFF образует 17 областей (плоскостей) по 0000-FFFF (0-65536), где первую область называют ОСНОВНОЙ ЯЗЫКОВОЙ ПЛОСКОСТЬЮ,
т.к. этот числовой диапазон покрывал основные символы основных языков для возникновения стандарта unicode. 

0xFFFF = 0b1111_1111_1111_1111 = 65536 = 2 байта = 16 бит (UTF-16 - кодиуется 2-х байтовыми словами)

== немного предистории ==
Собствено до Unicode была сначала базовая кодировка теста для латиницы - ASCII (Она описывает первые 128 символов из наиболее часто используемых англоязычными пользователями — латинские буквы, арабские цифры и знаки препинания)
и для этой кодировки было состаточно 1байта
Т.к. в 1байте можно хранить до 256 символов появились расширения ASCII: кодировки CP866 и KOI8-R с псевдографикой (Т.е. появилась возможность добавить в Аски символы букв своего языка)

Проблемы: 
1. Только лишь для кодирования символов русского языка существует несколько разновидностей расширенной Аски (Windows 1251 (убрали псевдографику за ненадобностью, т.е. появились графические ОС,
   и вместо ей заняли недостающие символы русской типографики, KOI8-R, CP866).
2. Одним байтом не обойтись, например, китайский язык имеет кучу иероглифов. Эти тысячи знаков языковой группы юго-восточной Азии никак невозможно было описать в одном байте информации,
   который выделялся для кодирования символов в расширенных версиях ASCII
  
В итоге было признано необходимым создание единой «широкой» кодировки. 
Изначально в Юникоде  планировалось кодировать НЕ все существующие символы, а только те, которые необходимы в повседневном обиходе
Использование 32-битных символов казалось слишком расточительным, поэтому было решено использовать 16-битные 
Таким образом, первая версия Юникода представляла собой кодировку с фиксированным размером символа в 16 бит, то есть общее число кодов было 216 (65 536). 
Отсюда происходит практика обозначения символов четырьмя шестнадцатеричными цифрами (например, U+04F0).

В дальнейшем, однако, было принято решение кодировать ВСЕ символы и в связи с этим значительно расширить кодовую область.
Коды символов стали рассматриваться не как 16-битные значения, а как абстрактные числа, которые в компьютере могут представляться множеством разных способов
Поскольку в ряде компьютерных систем (например, Windows NT[10]) фиксированные 16-битные символы уже использовались в качестве кодировки по умолчанию, 
было решено все наиболее важные знаки кодировать только в пределах первых 65 536 позиций (ОСНОВНОЙ ЯЗЫКОВОЙ ПЛОСКОСТЬЮ).
Остальное пространство используется для «дополнительных символов»

====Кодировка UTF-16 (символ кодируется либо 2 байтами, либо 4)====
Придумана для совместимости со старыми 16-битными системами, где первые 65 536 позиций, за исключением позиций из интервала U+D800…U+DFFF, 
отображаются непосредственно как 16-битные числа, а остальные представляются в виде «суррогатных пар»:
  - первый элемент пары из области U+D800…U+DBFF (55296-56319=1024кода или 2^10 ) 
  - второй элемент пары из области U+DC00…U+DFFF (1024 кода)
Итого: сурогантых пар м.б. 2^20 = 1048576 + 65536 кодов основного диапазона - 2048 зарезервированных кодов для сурогатных пар = 1 112 064 (доступное пространство кодовых точек unicode, см. выше)
Итого: символы в UTF-16 кодируются 2-х байтовыми словами (от 0 до FFFF) - КОДОВЫЕ ЕДИНИЦЫ, при этом можно кодировать символы Unicode в дипазонах 000016..D7FF16 и E00016..10FFFF16. 
если символ из запрещённого диапазона U+D800…U+DBFF, понимаем, что это сурогатная пара, т.е. символ кодируется 4-мя байтами.

(!) Итого: КОДОВЫМИ ЕДИНИЦАМИ кодирутся КОДОВЫЕ ЗНАЧЕНИЯ Unicode!! и одно кодовое значение в UTF-16 м.б. закодировано двумя кодовыми единицами.
Размер кодовой единицы зависит от кодировки, в UTF-16 имеют фиксированный размер 2-байта, в UTF-8 кодовая единица переменого размера (1-2-3-4-5-6 байт, см. ниже)!  

Почему это важно! Например, если в java работать с текстом UTF-16, то строка 
String str = "A" + "\uD835\uDD0A"+ "B" + "C";  
System.out.print(str);  // отобразит строку A@BC, а charAt(1) вернёт не символ @, а значение кодовой единицы в позиции 1 \uD835
System.out.print(" - length: "+str.length());  // длина 5 !!!!
System.out.print(" - codePointCount: "+str.codePointCount(0, str.length()));  // число кодовых точек 4 !!!! т.к. совокупность \uD835\uDD0A представляет один символ в UTF-16
		
фактически строка будет иметь 5 кодов! и длина её будет 5, поэтому, для корректного подсчета числа символов нужно считать количество коловых точек!!
codePointCount не для UTF-16 работает также как и length.
Обычные методы для работы с char можно использовать только при не иcпользовании UTF-16.

======== Кодировка UTF-8 (символ кодируется только необходимым количеством для него байтов)============
Символы ASCII  (до 128) будут закодированы 1 байтом, иначе выделяется столько байтов сколько нужно, по определённому правилу:
(1 байт)  0aaa aaaa 
(2 байта) 110x xxxx 10xx xxxx
(3 байта) 1110 xxxx 10xx xxxx 10xx xxxx
(4 байта) 1111 0xxx 10xx xxxx 10xx xxxx 10xx xxxx
(5 байт)  1111 10xx 10xx xxxx 10xx xxxx 10xx xxxx 10xx xxxx
(6 байт)  1111 110x 10xx xxxx 10xx xxxx 10xx xxxx 10xx xxxx 10xx xxxx
Первый байт содержит количество байтов символа, закодированное в единичной системе счисления
Далее «0» — бит терминатор, означающий завершение кода размера
Далее идут значащие байты кода, которые имеют вид (10xx xxxx), где «10» — биты признака продолжения, а x — значащие биты.
====


==== Кодировка в JVM (http://www.getinfo.ru/article296.html) ====
При компиляции java кодирует все символы в unicоde, и для корректного перевода в unicode кодировка java-исходника и кодировку которую использует компилятор д.б. одинаковыми!!!
char ch=189; // это значение Unicode
По дефолту javac использует системную кодировку или берёт её из ключа javac -encode.
Итого, внутри JVM все в unicode, т.е. все переменные char, string имеют фиксированную кодировку - Unicode

(!)НО когда данные читаются извне или передаются наружу, то они могут быть представлены только одним типом - типом byte. Например:
// Данные в кодировке КОИ-8
byte[] koi8Data = ...;
// Преобразуем из КОИ-8 в Unicode
String string = new String(koi8Data,"KOI8_R");
// Преобразуем из Unicode в Windows-1251
byte[] winData = string.getBytes("Cp1251");
// Строка Unicode
String string = "...";
// Преобразуем из Unicode в UnicodeLittleUnmarked
byte[] data = string.getBytes("UnicodeLittleUnmarked");

Всегда, когда читаем байты извне, при перекодировании байтов в строку нужно всегда указывать кодировку!

-- чтение с консоли --
InputStream is = ..;
int b;
ByteArrayOutputStream baos = new ByteArrayOutputStream();
while( (b=is.read())!=-1 )  {
   baos.write( b );
  }
// Перекодирование байтов в строку с использованием кодировки по умолчанию
String s = baos.toString();
// Если нужна конкретная кодировка - просто укажите её при вызове toString():
// s = baos.toString("Cp1251");

-- чтение потоков/файлов --
Тут есть правило: Если известно, что байты представляют собой только символы в некоторой кодировке, можно использовать специальные классы-преобразователи InputStreamReader и OutputStreamWriter,
чтобы получить поток символов и работать непосредственно с ним.
Обычно это удобно в случае обычных текстовых файлов или при работе с многими сетевыми протоколами Internet. Кодировка символов при этом указывается в конструкторе класса-преобразователя. Пример:
// Строка Unicode
String string = "...";
// Записываем строку в текстовый файл в кодировке Cp866
PrintWriter pw = new PrintWriter   // класс с методами записи строк
  (new OutputStreamWriter          // класс-преобразователь
     (new FileOutputStream         // класс записи байтов в файл
        ("file.txt"), "Cp866");
pw.println(string);  // записываем строку в файл
pw.close();

Если в потоке могут присутствовать данные в разных кодировках или же символы перемешаны с прочими двоичными данными, то лучше читать и записывать массивы байтов (byte[]),
 а для перекодировки использовать уже упомянутые методы класса String. Пример:

// Строка Unicode
String string = "...";
// Записываем строку в текстовый файл в двух кодировках (Cp866 и Cp1251)
OutputStream os = new FileOutputStream("file.txt"); // класс записи байтов в файл
// Записываем строку в кодировке Cp866
os.write( string.getBytes("Cp866") );
// Записываем строку в кодировке Cp1251
os.write( string.getBytes("Cp1251") );
os.close();

Официальные наименования кодировок, для указания в параметрах String charsetName см. описание Class Charset, а также http://www.iana.org/assignments/character-sets/character-sets.xhtml
Пример загрузки прочик кодировок (см. CharsetProvider): http://stackoverflow.com/questions/11800145/how-can-i-get-a-custom-charset-to-load-in-hadoop



